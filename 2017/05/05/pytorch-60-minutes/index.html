<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Gaussic"><title>Pytorch整理：60分钟入门 · 夜露</title><!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="description" content="由于种种原因，近段时间开始尝试使用Pytorch。照着官方给的教程慢慢搞，稍微有一点点理解。在这里做一点小小的记录和翻译工作。
官方地址：Deep Learning with PyTorch: A 60 Minute Blitz
感谢作者： Soumith Chintala
转载请说明出处：Gaus"><meta name="keywords" content="Machine Learning, Deep Learning, NLP, AI, 机器学习, 深度学习, 自然语言处理, 人工智能"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/github.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">夜露</a></h3><div class="description"><p>No good things ever dies.</p></div></div></div><ul class="social-links"><li><a href="https://github.com/gaussic"><i class="fa fa-github"></i></a></li><li><a href="http://weibo.com/2625944715"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"></a></li></div><div class="avatar"><img src="/images/avatar.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Pytorch整理：60分钟入门</a></h3></div><div class="post-content"><p>由于种种原因，近段时间开始尝试使用Pytorch。照着官方给的教程慢慢搞，稍微有一点点理解。在这里做一点小小的记录和翻译工作。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>官方地址：<a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">Deep Learning with PyTorch: A 60 Minute Blitz</a></p>
<p>感谢作者： <a href="http://soumith.ch/" target="_blank" rel="noopener">Soumith Chintala</a></p>
<p>转载请说明出处：<a href="https://gaussic.github.io/2017/05/05/pytorch-60-minutes/">Gaussic:夜露</a></p>
<p>这个教程的目标：</p>
<ul>
<li>更高层次地理解Pythrch的Tensor库以及神经网络。</li>
<li>训练一个小的神经网络模型用于分类图像。</li>
</ul>
<h3 id="什么是Pytorch"><a href="#什么是Pytorch" class="headerlink" title="什么是Pytorch"></a>什么是Pytorch</h3><p>这是一个基于Python的科学计算包，主要针对两类人群：</p>
<ul>
<li>替代Numpy以发挥GPU的强大能力</li>
<li>一个提供最大灵活性和速度的深度学习研究平台</li>
</ul>
<h3 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h3><h4 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h4><p>Tensors类似于numpy的ndarray，但是带了一些附加的功能，例如可以使用GPU加速计算等等。</p>
<p>构建一个未初始化的 5x3 矩阵：</p>
<pre><code class="python">import torch
x = torch.Tensor(5, 3)
print(x)
</code></pre>
<pre><code class="python">1.00000e-28 *
  0.0000  0.2524  0.0000
  0.2524  2.8715  0.0000
  2.9158  0.0000  2.9157
  0.0000  2.9158  0.0000
  0.0003  0.0000  0.0000
[torch.FloatTensor of size 5x3]
</code></pre>
<p>构建一个随机初始化的矩阵：</p>
<pre><code class="python">x = torch.rand(5, 3)
print(x)
</code></pre>
<pre><code class="python"> 0.5453  0.4855  0.7236
 0.3199  0.4525  0.4917
 0.6965  0.8742  0.9948
 0.9029  0.1873  0.0018
 0.3080  0.2953  0.4313
[torch.FloatTensor of size 5x3]
</code></pre>
<p>获取矩阵维度大小：</p>
<pre><code class="python">print(x.size())
</code></pre>
<pre><code class="python">torch.Size([5, 3])
</code></pre>
<blockquote>
<p>注意：</p>
<p><code>torch.Size</code> 实际上是一个元组，因此它支持相同的操作。</p>
</blockquote>
<h4 id="运算操作"><a href="#运算操作" class="headerlink" title="运算操作"></a>运算操作</h4><p>运算操作有多种语法，让我们看看加法的例子。</p>
<p>加法：语法1</p>
<pre><code class="python">y = torch.rand(5, 3)
print(x + y)
</code></pre>
<pre><code class="python"> 1.1177  0.8514  1.1459
 1.1878  0.9249  0.5759
 1.3508  1.4628  1.2833
 1.8678  0.8499  0.2941
 0.9718  1.0785  0.6914
[torch.FloatTensor of size 5x3]
</code></pre>
<p>加法：语法2</p>
<pre><code class="python">print(torch.add(x, y))
</code></pre>
<pre><code class="python"> 1.1177  0.8514  1.1459
 1.1878  0.9249  0.5759
 1.3508  1.4628  1.2833
 1.8678  0.8499  0.2941
 0.9718  1.0785  0.6914
[torch.FloatTensor of size 5x3]
</code></pre>
<p>加法：给定一个输出tensor</p>
<pre><code class="python">result = torch.Tensor(5, 3)
torch.add(x, y, out=result)
print(result)
</code></pre>
<pre><code class="python"> 1.1177  0.8514  1.1459
 1.1878  0.9249  0.5759
 1.3508  1.4628  1.2833
 1.8678  0.8499  0.2941
 0.9718  1.0785  0.6914
[torch.FloatTensor of size 5x3]
</code></pre>
<p>加法：就地(in-place)</p>
<pre><code class="python"># adds x to y
y.add_(x)
print(y)
</code></pre>
<pre><code class="python"> 1.1177  0.8514  1.1459
 1.1878  0.9249  0.5759
 1.3508  1.4628  1.2833
 1.8678  0.8499  0.2941
 0.9718  1.0785  0.6914
[torch.FloatTensor of size 5x3]
</code></pre>
<blockquote>
<p>注意：<br>任何就地改变一个tensor的操作都以<code>_</code>为后缀。例如：<code>x.copy_(y)</code>, <code>x.t_()</code>，都会改变<code>x</code>。</p>
</blockquote>
<p>你可以像numpy一样使用索引!</p>
<pre><code class="python">print(x[:, 1])
</code></pre>
<pre><code class="python"> 0.4855
 0.4525
 0.8742
 0.1873
 0.2953
[torch.FloatTensor of size 5]
</code></pre>
<p><strong>延伸阅读</strong>：</p>
<p>100+ Tensor运算，包括转置、索引、切分、数学运算、线性代数随机数等等，链接：<a href="http://pytorch.org/docs/torch" target="_blank" rel="noopener">戳这</a></p>
<h3 id="Numpy的桥梁"><a href="#Numpy的桥梁" class="headerlink" title="Numpy的桥梁"></a>Numpy的桥梁</h3><p>Torch的Tensor和Numpy的数组之间的互转简直像一阵清风一样。</p>
<p>Torch的Tensor和Numpy的数组会共享它们的底层存储位置，该变其中一个，另外一个也会改变。</p>
<h4 id="将Torch-Tensor转换为Numpy数组"><a href="#将Torch-Tensor转换为Numpy数组" class="headerlink" title="将Torch Tensor转换为Numpy数组"></a>将Torch Tensor转换为Numpy数组</h4><pre><code class="python">a = torch.ones(5)
print(a)
</code></pre>
<pre><code class="python"> 1
 1
 1
 1
 1
[torch.FloatTensor of size 5]
</code></pre>
<pre><code class="python">b = a.numpy()
print(b)
</code></pre>
<pre><code class="python">[ 1.  1.  1.  1.  1.]
</code></pre>
<p>看看当改变numpy数组的值时发生了什么。</p>
<pre><code class="python">a.add_(1)
print(a)
print(b)
</code></pre>
<pre><code class="python"> 2
 2
 2
 2
 2
[torch.FloatTensor of size 5]

[ 2.  2.  2.  2.  2.]
</code></pre>
<h4 id="将Numpy数组转换为Torch-Tensor"><a href="#将Numpy数组转换为Torch-Tensor" class="headerlink" title="将Numpy数组转换为Torch Tensor"></a>将Numpy数组转换为Torch Tensor</h4><p>看看更改Numpy数组的同时自动地更改了Torch Tensor</p>
<pre><code class="python">import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)
</code></pre>
<pre><code class="python">[ 2.  2.  2.  2.  2.]

 2
 2
 2
 2
 2
[torch.DoubleTensor of size 5]
</code></pre>
<p>除了CharTensor之外，CPU上的所有Tensor都支持与Numpy数组的来回转换。</p>
<h4 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h4><p>可以使用<code>.cuda</code>函数将Tensor转移到GPU上。</p>
<pre><code class="python"># let us run this cell only if CUDA is available
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
</code></pre>
<h3 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd:自动求导"></a>Autograd:自动求导</h3><p>Pytorch中所有神经网络的中心部分是<code>autograd</code>包。我们首先浏览一下它，然后再构建我们的第一个神经网络。</p>
<p><code>autograd</code>包为Tensor上的所有运算提供了自动求导功能。它是一个由运行定义的框架，即你的反向传播是由你的代码如何运行来决定的，而且每一轮迭代都可能是不同的。</p>
<p>让我们用几个简单的例子来了解几个简单的术语。</p>
<h4 id="Variable-变量"><a href="#Variable-变量" class="headerlink" title="Variable 变量"></a>Variable 变量</h4><p><code>autograd.Variable</code>是这个包的中心类。它包装一个Tensor，并且支持几乎所有定义在这个Tensor上的运算。一旦你完成了你的计算，你可以调用<code>.backward()</code>来自动地计算全部的梯度。</p>
<p>你可以通过<code>.data</code>属性来访问最原始的tensor，而梯度则相应地被累计到了<code>.grad</code>中。</p>
<p><img src="/2017/05/05/pytorch-60-minutes/Variable.png" alt="pyt"></p>
<p>autograd的实现中还有一个非常重要的类-<code>Function</code>。</p>
<p><code>Variable</code>和<code>Function</code>是相互关联的并且构建了一个非循环图，其中编码了整个的计算历史。每一个变量都有一个<code>.creator</code>属性，它引用一个常见<code>Variable</code>的<code>Function</code>（除了用户创建的Variables-它们的<code>creator是None</code>）。</p>
<p>如果你想计算导数，可以在<code>Variable</code>上调用<code>.backward()</code>。如果<code>Variable</code>是个标量（一个单元素数据），那么你不用为<code>backward()</code>指定任何参数，然而如果它有多个元素，你需要指定一个<code>grad_output</code>参数，它是一个匹配尺寸的tensor。</p>
<pre><code class="python">import torch
from torch.autograd import Variable
</code></pre>
<p>创建一个变量：</p>
<pre><code class="python">x = Variable(torch.ones(2, 2), requires_grad=True)
print(x)
</code></pre>
<pre><code class="python">Variable containing:
 1  1
 1  1
[torch.FloatTensor of size 2x2]
</code></pre>
<p>对变量进行运算：</p>
<pre><code class="python">y = x + 2
print(y)
</code></pre>
<pre><code class="python">Variable containing:
 3  3
 3  3
[torch.FloatTensor of size 2x2]
</code></pre>
<p><code>y</code>是作为一个运算操作的结果而创建的，因而它有一个creator</p>
<pre><code class="python">print(y.creator)
</code></pre>
<pre><code class="python">&lt;torch.autograd._functions.basic_ops.AddConstant object at 0x108ada4a8&gt;
</code></pre>
<p>在y上做更多的运算：</p>
<pre><code class="python">z = y * y * 3
out = z.mean()

print(z, out)
</code></pre>
<pre><code class="python">Variable containing:
 27  27
 27  27
[torch.FloatTensor of size 2x2]
 Variable containing:
 27
[torch.FloatTensor of size 1]
</code></pre>
<h4 id="Gradients-梯度"><a href="#Gradients-梯度" class="headerlink" title="Gradients 梯度"></a>Gradients 梯度</h4><p>让我们使用反向传播<code>out.backward()</code>，它等同于<code>out.backward(torch.Tensor([1.0]))</code>。</p>
<pre><code class="python">out.backward()
</code></pre>
<p>打印梯度 d(out)/dx：</p>
<pre><code class="python">print(x.grad)
</code></pre>
<pre><code class="python">Variable containing:
 4.5000  4.5000
 4.5000  4.5000
[torch.FloatTensor of size 2x2]
</code></pre>
<p>你应该会得到一个<code>4.5</code>的矩阵。让我们称<code>out</code>变量为<em>o</em>。我们有</p>
<p>$$o = \frac{1}{4}\sum_i z_i$$</p>
<p>$$z_i = 3(x_i+2)^2$$</p>
<p>$$ z_i\bigr\rvert_{x_i=1} = 27 $$</p>
<p>因此，</p>
<p>$$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$$</p>
<p>$$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$$</p>
<p>你还可以使用autograd做一些疯狂的事情！</p>
<pre><code class="python">x = torch.randn(3)
x = Variable(x, requires_grad=True)

y = x * 2
while y.data.norm() &lt; 1000:
    y = y * 2

print(y)
</code></pre>
<pre><code class="python">Variable containing:
 596.2775
-807.4459
-550.6819
[torch.FloatTensor of size 3]
</code></pre>
<pre><code class="python">gradients = torch.FloatTensor([0.1, 1.0, 0.0001])
y.backward(gradients)

print(x.grad)
</code></pre>
<pre><code class="python">Variable containing:
  102.4000
 1024.0000
    0.1024
[torch.FloatTensor of size 3]
</code></pre>
<p>延伸阅读：</p>
<p><code>Variable</code>和<code>Function</code>的文档：<a href="http://pytorch.org/docs/autograd" target="_blank" rel="noopener">http://pytorch.org/docs/autograd</a></p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络可以使用<code>torch.nn</code>包来构建。</p>
<p>现在你大致了解了<code>autograd</code>，<code>nn</code>依赖于<code>autograd</code>来定义模型并进行求导。一个<code>nn.Module</code>包含多个神经网络层，以及一个<code>forward(input)</code>方法来返回<code>output</code>。</p>
<p>例如，看看以下这个分类数字图像的网络：</p>
<p><img src="/2017/05/05/pytorch-60-minutes/mnist.png" alt="mnist"></p>
<p>它是一个简单的前馈网络。它将输入逐步地喂给多个层，然后给出输出。</p>
<p>一个典型的神经网络训练过程如下：</p>
<ul>
<li>定义一个拥有可学习参数（或权重）的神经网络</li>
<li>在输入数据上进行迭代</li>
<li>在网络中处理数据</li>
<li>计算损失（输出离分类正确有多远）</li>
<li>梯度反向传播给网络的参数</li>
<li>更新网络的权重，通常使用一个简单的更新规则：<code>weight = weight + learning_rate * gradient</code></li>
</ul>
<h4 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h4><pre><code class="python">import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 图像输入通道, 6 输出通道, 5x5 正方形卷积核
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 使用 (2, 2) 窗口最大池化
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]   # 所有维度，除了批尺寸
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

net = Net()
print(net)
</code></pre>
<pre><code class="python">Net (
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear (400 -&gt; 120)
  (fc2): Linear (120 -&gt; 84)
  (fc3): Linear (84 -&gt; 10)
)
</code></pre>
<p>你只需要定义<code>forward</code>函数，<code>backward</code>函数（用来计算梯度）是使用<code>autograd</code>自动为你定义的。你可以在<code>forward</code>中使用任意的Tensor运算操作。</p>
<p>模型中可学习的参数是通过<code>net.parameters()</code>返回的：</p>
<pre><code class="python">params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1&#39;s .weight
</code></pre>
<pre><code class="python">10
torch.Size([6, 1, 5, 5])
</code></pre>
<p>forward的输入是一个<code>autograd.Variable</code>    ，输出亦然。</p>
<pre><code class="python">input = Variable(torch.randn(1, 1, 32, 32))
out = net(input)
print(out)
</code></pre>
<pre><code class="python">Variable containing:
 0.0455 -0.0445  0.0064 -0.0310  0.0945 -0.0362 -0.1971  0.0555  0.0943  0.1016
[torch.FloatTensor of size 1x10]
</code></pre>
<p>将梯度缓冲区置0，并使用随机的梯度进行反向传播：</p>
<pre><code class="python">net.zero_grad()
out.backward(torch.randn(1, 10))
</code></pre>
<blockquote>
<p>注意：</p>
<p><code>torch.nn</code>仅支持mini-batch。整个的<code>torch.nn</code>包仅支持小批量的数据，而不是一个单独的样本。</p>
<p>例如，<code>nn.Conv2d</code>应传入一个4D的Tensor，维度为<code>nSamples x nChannels x Height x Width</code>。</p>
<p>如果你有一个单独的样本，使用<code>input.unsqueeze(0)</code>来添加一个伪批维度。</p>
</blockquote>
<p>在继续之前，我们先回顾一下迄今为止的所有课程。</p>
<p>回顾：</p>
<ul>
<li><code>torch.Tensor</code> 一个多维数组</li>
<li><code>autograd.Variable</code> 包装一个Tensor并且记录应用在其上的历史运算操作。拥有与<code>Tensor</code>相同的API，添加了一些像<code>backward()</code>的操作。还包括相关tensor的梯度。</li>
<li><code>nn.Module</code> 神经网络模块。封装参数的方便方式，带有将它们转移到GPU、导出、载入等的帮助函数。</li>
<li><code>nn.Parameter</code> 一种Variable，当给<code>Module</code>赋值时自动注册一个参数。</li>
<li><code>autograd.Function</code> 实现一个autograd 操作的 forward 和 backward 定义。每一个<code>Variable</code>操作，创建至少一个<code>Function</code>节点，来连接那些创建<code>Variable</code>的函数，并且记录其历史。</li>
</ul>
<p>在这里，我们涵盖了：</p>
<ul>
<li>定义神经网络</li>
<li>处理输入并调用backward</li>
</ul>
<p>还剩下：</p>
<ul>
<li>计算损失</li>
<li>更新网络权重</li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>一个损失函数以一个(output, target)对为输入，然后计算一个值用以估计输出结果离目标结果多远。</p>
<p>存在多种的<a href="http://pytorch.org/docs/nn.html#loss-functions" target="_blank" rel="noopener">损失函数</a>。一个简单的损失函数：<code>nn.MSELoss</code>，它计算输出与目标的均方误差。</p>
<p>例如：</p>
<pre><code class="python">output = net(input)
target = Variable(torch.arange(1, 11))  # a dummy target, for example
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
</code></pre>
<pre><code class="python">Variable containing:
 38.3005
[torch.FloatTensor of size 1]
</code></pre>
<p>现在，如果你在反方向跟随<code>loss</code>，使用它的<code>.creator</code>属性，你会看到一个如下所示的计算图：</p>
<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
      -&gt; MSELoss
      -&gt; loss
</code></pre><p>因此，当我们调用<code>loss.backward()</code>时，损失对应的整个图都被求导，并且图中所有的变量都会带有累积了梯度的<code>.grad</code>属性。</p>
<pre><code class="python">print(loss.creator)  # MSELoss
print(loss.creator.previous_functions[0][0])  # linear
print(loss.creator.previous_functions[0][0].previous_functions[0][0])  # ReLU
</code></pre>
<pre><code class="python">&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x10e0bdd68&gt;
&lt;torch.nn._functions.linear.Linear object at 0x10e0bdba8&gt;
&lt;torch.nn._functions.thnn.auto.Threshold object at 0x10e0bdac8&gt;
</code></pre>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>要进行反向传播，我们只需要调用<code>loss.backward()</code>。你需要清除现有的梯度，否则梯度将累积到现有梯度。</p>
<p>现在我们将调用<code>loss.backward()</code>，并看看conv1在backward之前和之后的梯度变化。</p>
<pre><code class="python">net.zero_grad()   # zeroes the gradient buffers of all parameters

print(&#39;conv1.bias.grad before backward&#39;)
print(net.conv1.bias.grad)

loss.backward()

print(&#39;conv1.bias.grad after backward&#39;)
print(net.conv1.bias.grad)
</code></pre>
<pre><code class="python">conv1.bias.grad before backward
Variable containing:
 0
 0
 0
 0
 0
 0
[torch.FloatTensor of size 6]

conv1.bias.grad after backward
Variable containing:
 0.1392
-0.1155
 0.0247
 0.1121
-0.0559
 0.0363
[torch.FloatTensor of size 6]
</code></pre>
<p>现在我们知道怎么使用损失函数了。</p>
<p><em>延伸阅读</em></p>
<p>神经网络包包含构建深度神经网络的多个模块和损失函数。一个完整的文档列表<a href="http://pytorch.org/docs/nn" target="_blank" rel="noopener">在这里</a></p>
<p>仅剩的一个要学习的东西：</p>
<ul>
<li>更新网络权重</li>
</ul>
<h4 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h4><p>实践中最简单的更新规则是随机梯度下降（SGD）：</p>
<pre><code class="python">weight = weight - learning_rate * gradient
</code></pre>
<p>我们可以使用简单的Python代码实现：</p>
<pre><code class="python">learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
</code></pre>
<p>然而，当您使用神经网络时，您希望使用各种不同的更新规则，例如SGD，Nesterov-SGD，Adam，RMSProp等等。为了实现这一点，我们构建一个小的包：<code>torch.optim</code>，来实现所有的方法。使用非常简单：</p>
<pre><code class="python">import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
</code></pre>
<h3 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h3><p>在此，你已经知道如何定义神经网络，计算损失以及更新网络权重。</p>
<p>现在你可能会想，</p>
<h4 id="数据怎么办"><a href="#数据怎么办" class="headerlink" title="数据怎么办"></a>数据怎么办</h4><p>一般来说，当你处理图像、文本、音频或视频数据时，你可以使用标准的python包来将数据载入到numpy数组中。然后你可以将这个数组转化为<code>torch.Tensor</code>。</p>
<ul>
<li>对于图像，诸如Pillow, OpenCV这些包很好用。</li>
<li>对于音频，可以使用scipy和librosa。</li>
<li>对于文本，要么使用原始的Python或Cython载入方式，要么使用NLTK和SpaCy。</li>
</ul>
<p>特别的对于<code>vision</code>，我们创建了一个叫做<code>torchvision</code>的包，它有一些常用数据集（Imagenet, CIFAR10, MNIST等）的数据载入器，以及图像的数据转换器, <code>torchvision.datasets</code>和<code>torch.utils.data.DataLoader</code>。</p>
<p>这提供了巨大的便利，避免编写样板代码。</p>
<p>在本教程中，我们使用CIFAR10数据集。它有10个类别：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。CIFAR10中的图像尺寸在3x32x32，即3通道彩色图像，32x32像素大小。</p>
<p><img src="/2017/05/05/pytorch-60-minutes/cifar10.png" alt="cifar10"></p>
<h3 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h3><p>我们将按顺序完成以下步骤：</p>
<ol>
<li>载入和规范化CIFAR10的训练和测试集，使用<code>torchvision</code></li>
<li>定义一个卷积神经网络</li>
<li>定义损失函数</li>
<li>在训练集上进行训练</li>
<li>在测试集上测试网络</li>
</ol>
<h4 id="1-载入和规范化CIFAR10"><a href="#1-载入和规范化CIFAR10" class="headerlink" title="1. 载入和规范化CIFAR10"></a>1. 载入和规范化CIFAR10</h4><p>使用<code>torchvision</code>，载入CIFAR10非常简单。</p>
<pre><code class="python">import torch
import torchvision
import torchvision.transforms as transforms
</code></pre>
<p>torchvision datasets的输出时范围在[0, 1]的PILImage图像。我们将它们转换为规范区间[-1, 1]的Tensor。</p>
<pre><code class="python">transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True,
                                       download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False,
                                       download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=True, num_workers=2)

classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)
</code></pre>
<p>让我们展示一些训练图像。</p>
<pre><code class="python">import matplotlib.pyplot as plt
import numpy as np

# functions to show image
def imshow(img):
    img = img / 2 + 0.5 # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

# get some random training images
dataiter = iter(trainloader)
images, labels = dataiter.next()

# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))
</code></pre>
<p><img src="/2017/05/05/pytorch-60-minutes/cifar_train_1.png" alt="cifar_train_1"></p>
<h4 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2. 定义一个卷积神经网络"></a>2. 定义一个卷积神经网络</h4><p>复制在神经网络那一节的神经网络，将其更改为3通道图像输入（而不是原始的单通道输入）。</p>
<pre><code class="python">from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()
</code></pre>
<h4 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3. 定义损失函数和优化器"></a>3. 定义损失函数和优化器</h4><p>让我们来使用分类交叉熵和带有动量的SGD</p>
<pre><code class="python">import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
</code></pre>
<h4 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4. 训练网络"></a>4. 训练网络</h4><pre><code class="python">for epoch in range(10): # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the input
        inputs, labels = data

        # wrap time in Variable
        inputs, labels = Variable(inputs), Variable(labels)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.data[0]
        if i % 2000 == 1999:   # print every 2000 mini-batches
            print(&#39;[%d, %5d] loss: %.3f&#39; %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print(&#39;Finished Training&#39;)
</code></pre>
<pre><code class="python">[1,  2000] loss: 1.184
[1,  4000] loss: 1.206
[1,  6000] loss: 1.186
[1,  8000] loss: 1.162
[1, 10000] loss: 1.195
[1, 12000] loss: 1.165
[2,  2000] loss: 1.095
[2,  4000] loss: 1.076
[2,  6000] loss: 1.086
[2,  8000] loss: 1.092
[2, 10000] loss: 1.060
[2, 12000] loss: 1.110
[3,  2000] loss: 0.999
[3,  4000] loss: 1.005
[3,  6000] loss: 1.016
[3,  8000] loss: 1.016
[3, 10000] loss: 1.017
[3, 12000] loss: 1.023
[4,  2000] loss: 0.922
[4,  4000] loss: 0.933
[4,  6000] loss: 0.959
[4,  8000] loss: 0.975
[4, 10000] loss: 0.985
[4, 12000] loss: 0.968
[5,  2000] loss: 0.861
[5,  4000] loss: 0.908
[5,  6000] loss: 0.911
[5,  8000] loss: 0.932
[5, 10000] loss: 0.920
[5, 12000] loss: 0.919
[6,  2000] loss: 0.839
[6,  4000] loss: 0.853
[6,  6000] loss: 0.887
[6,  8000] loss: 0.891
[6, 10000] loss: 0.890
[6, 12000] loss: 0.876
[7,  2000] loss: 0.819
[7,  4000] loss: 0.808
[7,  6000] loss: 0.831
[7,  8000] loss: 0.852
[7, 10000] loss: 0.842
[7, 12000] loss: 0.869
[8,  2000] loss: 0.761
[8,  4000] loss: 0.784
[8,  6000] loss: 0.808
[8,  8000] loss: 0.827
[8, 10000] loss: 0.841
[8, 12000] loss: 0.860
[9,  2000] loss: 0.731
[9,  4000] loss: 0.758
[9,  6000] loss: 0.801
[9,  8000] loss: 0.784
[9, 10000] loss: 0.831
[9, 12000] loss: 0.817
[10,  2000] loss: 0.723
[10,  4000] loss: 0.733
[10,  6000] loss: 0.775
[10,  8000] loss: 0.763
[10, 10000] loss: 0.802
[10, 12000] loss: 0.799
Finished Training
</code></pre>
<h4 id="5-在测试数据上测试网络"><a href="#5-在测试数据上测试网络" class="headerlink" title="5. 在测试数据上测试网络"></a>5. 在测试数据上测试网络</h4><p>我们已经在训练集上训练了10轮。但是我们需要检查网络是否有学到什么。</p>
<p>我们可以通过检测预测的类别标签，再与真实标签进行对比。如果预测是对的，我们将这个样本加到分类正确的列表中。</p>
<p>Okay，第一步。让我们先展示一些测试数据集中的图像。</p>
<pre><code class="python">dataiter = iter(testloader)
images, labels = dataiter.next()

# print images
imshow(torchvision.utils.make_grid(images))
print(&#39;GroundTruth: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))
</code></pre>
<p><img src="/2017/05/05/pytorch-60-minutes/cifar_test_1.png" alt="cifar_test_1"></p>
<p>再来看一下神经网络认为这些样本是什么。</p>
<p>输出是10个类别的能量。一个类别能量越高，网络就更多地认为图像是这个特定的类别。因此，让我们获取最高能量类别的索引。</p>
<pre><code class="python">outputs = net(Variable(images))

_, predicted = torch.max(outputs.data, 1)

print(&#39;Predicted: &#39;, &#39; &#39;.join(&#39;%5s&#39; % classes[predicted[j][0]] for j in range(4)))
</code></pre>
<pre><code class="python">Predicted:  horse plane horse  frog
</code></pre>
<p>结果看起来不错。</p>
<p>让我们再来看看网络在整个数据集上的性能。</p>
<pre><code class="python">correct = 0
total = 0
for data in testloader:
    images, labels = data
    outputs = net(Variable(images))
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum()

print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total))
</code></pre>
<pre><code class="python">Accuracy of the network on the 10000 test images: 63 %
</code></pre>
<p>这个结果看起来远比随机抽取要好，随机抽取的概率为10%。看起来网络确实学到了一些东西。</p>
<p>那么，有哪些类别表现优秀，哪些类别表现不佳呢？</p>
<pre><code class="python">class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
for data in testloader:
    images, labels = data
    outputs = net(Variable(images))
    _, predicted = torch.max(outputs.data, 1)
    c = (predicted == labels).squeeze()
    for i in range(4):
        label = labels[i]
        class_correct[label] += c[i]
        class_total[label] += 1

for i in range(10):
    print(&#39;Accuracy of %5s: %2d %%&#39; % (classes[i], 100 * class_correct[i] / class_total[i]))
</code></pre>
<pre><code class="python">Accuracy of plane: 59 %
Accuracy of   car: 73 %
Accuracy of  bird: 51 %
Accuracy of   cat: 46 %
Accuracy of  deer: 51 %
Accuracy of   dog: 54 %
Accuracy of  frog: 76 %
Accuracy of horse: 69 %
Accuracy of  ship: 78 %
Accuracy of truck: 72 %
</code></pre>
<p>Okay，还有什么要说明的？</p>
<p>如何在GPU上面运行这个神经网络？</p>
<h3 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h3><p>与你如何将Tensor转移到GPU上类似，你可以将神经网络转移到GPU上。这将递归的遍历所有的模块并将它们的参数和缓存转化为CUDA tensors。</p>
<pre><code class="python">net.cuda()
</code></pre>
<p>记住，你还必须在每一步将输入和结果数据传输到GPU上：</p>
<pre><code class="python">inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())
</code></pre>
<p>为什么我没有注意到相比CPU的巨大的加速？因为你的神经网络非常小。</p>
<p><strong>训练</strong>：试着增加你的网络宽度（将第一个<code>nn.Conv2d</code>增广2，将第二个<code>nn.Conv2d</code>增广1 - 它们需要相同的数量），看看你的网络提速了多少。</p>
<p><strong>目标达成：</strong></p>
<ul>
<li>理解Pytorch的Tensor库以及高层次的神经网络</li>
<li>训练一个小的神经网络来分类图像</li>
</ul>
<p>转载请说明出处：<a href="https://gaussic.github.io/2017/05/05/pytorch-60-minutes/">Gaussic:夜露</a></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2017-05-05</span><i class="fa fa-tag"></i><a href="/categories/Deep-Learning/" title="Deep Learning" class="tag">Deep Learning </a><a href="/tags/Python/" title="Python" class="tag">Python </a><a href="/tags/Pytorch/" title="Pytorch" class="tag">Pytorch </a><a href="/tags/Tutorial/" title="Tutorial" class="tag">Tutorial </a></div></div></div></div><div class="share"><div class="evernote"> <a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"> <a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"> <a href="http://twitter.com/home?status=,https://gaussic.github.io/2017/05/05/pytorch-60-minutes/,夜露,Pytorch整理：60分钟入门,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2017/08/08/tf-idf-generation/" title="TF-IDF关键词提取实现" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2017/03/03/imdb-sentiment-classification/" title="深度神经网络训练IMDB情感分类的四种方法" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>